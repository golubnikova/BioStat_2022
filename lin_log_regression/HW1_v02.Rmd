---
title: "Домашнее задание по многофакторному анализу №1"
author: "Liliya Golubnikova"
date: "2023-01-23"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, message = FALSE)
```

## Загрузка библиотек
```{r}
#install.packages("faux")
#install.packages("olsrr")
library(MASS)
library(faux)
library(ggplot2)
library(dplyr)
library(tidyr)
library(faux)
library(tidyverse)
library(ggpubr)
library("olsrr")

```


## Задача 1
*Размер выборки*: s = 100  
*Показатели:*  
- креатинин `k`  
- мочевина `m`  

*Выборочные характеристики:*  
Средние значения:   
- `median(k) = 88.5`  
- `median(m) = 5.4`  
Стандартные отклонения:  
- `sd(k) = 13.25`  
- `sd(m) = 1.45`  
Коэффициент корреляции: `cor(k,m) = 0.6`  


### Моделируем выборку
```{r}
set.seed(1)
dat <- rnorm_multi(n = 100, 
                  mu = c(88.5, 5.4),
                  sd = c(13.25, 1.45),
                  r = c(0.6), 
                  varnames = c("k", "m"),
                  empirical = FALSE)

dat
```



### Оценим получившуюся модель
```{r}
summary(dat)
```


```{r}
cor(dat)
```


### Визуализация исходных данных
```{r}
#head(dat, 4)

theme_set(theme_pubr())
ggplot(dat, aes(x = k, y = m)) +
  geom_point() +
  stat_smooth()

```

### Построим модель линейной регрессии креатинина k на мочевину m
```{r}
# Линейная модель
model <- lm(m ~ k, data = dat)
summary(model)

ggplot(dat, aes(k, m)) +
  geom_point() +
  stat_smooth(method = lm)

```

*Анализ результата:*     
1) Уровень значимости статистической модели: уровень p-value = 1.332e-10, что меньше 0.05 => модель статистически значима.   
2) R^2 = 0.345 => 34,5% вариации переменной `m` можно объяснить переменной-предиктором `k`.  
3) Оценка коэффициента `k` = 0.059255. Каждое дополнительное увеличение `k` на 1 единицу связано со средним увеличением `m` на 0.059255.  
4) Cоласно значению p-value переменная `k` является статистически значимой (`p-value = 1.33e-10`).    
**Оценочное уравнение регрессии:**  `m = 0.240422 + 0.059255*(k)`.       


### Построим диагностические графики по полученной модели линейной регрессии:   

```{r}
plot(model)
```

```{r}
ols_plot_resid_box(model)
```


1) Для оценки наличия влиятельных наблюдений оценим *график остатков кредитного плеча (Residuals vs Leverage)*: наблюдение №10 ближе всего находится к границе расстояния Кука, но не выходит за пределы пугктирной линии => в нашем наборе нет чрезмерно влиятельных точек.   
2) Для проверки предположения о ровной дисперсии ("гомоскедастичности") остатков в регрессионной модели оценим *график масштаба-предположения (Scale-Location)*: красная линия проходит не совсем горизонтально на графике, но она не отклоняется слишком сильно ни в одной точке => предположение о равной дисперсии не нарушается.   
3) Для проверки нормальности распределения остатков регрессионной модели оценим *график Normal Q-Q*: большинство точек попадает на диагональную линию, хотя, наблюдаются некоторые отклонения. Таким образом, согласно графику нельзя считать, что распределение остатков нормальное.      
4) Оценим нормальность распределения с помощью *графика Residual box-plot*: остатки находятся вокруг нуля, усы почти одинакового размера, есть выбросы (отмечены зеленым), но незначительные. Делаем вывод, что распределение нормальное.   
5) Для определения того, имеют ли остатки нелинейные закономерности оценим *график Residuals vs Fitted*: красная линия отклоняется от идеальной горизонтальной линии, но не сильно. Можем сделать вывод, что  остатки следуют примерно линейному образцу и что для этого набора данных подходит модель линейной регрессии.   



### Проверка гипотезы о нормальном распределении остатков с помощью статистических тестов

**Гипотеза**   
**H0:** Распределение нормальное.   
**H1:** Распределение не нормальное.   



```{r}
ols_test_normality(model)
```

*Примечание:  тип используемого теста зависит от количества наблюдений. Так как у нас более 50 наблюдений, воспользуемся результатами Колмогорова-Смирнова (Kolmogorov-Smirnov).*      


*Результат:*    
Уровень значимости `p-value = 0.9158`, что больше 0.05 => *H0 принимается* => Распределение нормальное.    

**Общий вывод:**  на основе выполненных тестов можно считать, что распределение остатков нормальное, модель гомоскедастична, для данного набора данных подходит модель линейной регрессии.    



#### Как полученные оценки связаны с выборочными характеристиками (средним, дисперсией, корреляцией) выборки S?

1) Коэффициент детерминации (R-квадрат) — это доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью зависимости, то есть объясняющими переменными. В частности, для модели парной линейной регрессии коэффициент детерминации равен квадрату обычного коэффициента корреляции между y и x. В нашем случае R^2 = 0.3016, а cor(k,m) = 0.6.   

2) Наилучшая прямая линейной регрессии проходит через точку ($\overline{x}$; $\overline{y}$) и имеет угловой коэффициент и свободный член, соответственной:

$\alpha$ = ( $r_{xy}$ * $s_{y}$ ) / $s_{x}$, где $r_{xy}$ — коэффициент корреляции   

$\beta$ = $\overline{y}$ - $\overline{x}$ * $\alpha$  

#### Добавим в модель признак W, который не связан с k, m   


```{r}
set.seed(1)
dat2 <- rnorm_multi(n = 100, 
                  mu = c(88.5, 5.4, 5),
                  sd = c(13.25, 1.45, 2),
                  r = c(0.6, 0, 0), 
                  varnames = c("k", "m", "W"),
                  empirical = FALSE)

dat2
```



### Оценим получившуюся модель
```{r}
summary(dat2)
```


```{r}
cor(dat2)
```


### Визуализация исходных данных
```{r}
#head(dat, 4)

theme_set(theme_pubr())
ggplot(dat2, aes(x = k + W, y = m)) +
  geom_point() +
  stat_smooth()
```

### Построим модель линейной регрессии креатинина и признака (W): k + W на мочевину m
```{r}
# Линейная модель
model <- lm(m ~ k + W, data = dat2)
summary(model)

ggplot(dat2, aes(k + W, m)) +
  geom_point() +
  stat_smooth(method = lm)

```

*Анализ результата:*       
1) Уровень значимости статистической модели: уровень p-value = 1.5e-08, что меньше 0.05 => модель статистически значима. Вывод не изменился по сравнению с ранее построенной моделью.     
2) R^2 = 0.3103 => 31% вариации переменной `m` можно объяснить переменными-предикторами `k`, `W`. В модели ранее построенной модели (`m = 0.240422 + 0.059255*(k)`) R^2 выше (34%). Аналогично модифицированный коэффициент детерминации, в первой модели `Adjusted R^2 = 0.3383`. В новой модели `Adjusted R^2 = 0.296` -> снизился.       
3) Важно отметить, что переменная `k` является статистически значимой (`p-value = 2.32e-09`), а переменная `W` (`p-value = 0.626`) - нет.   





Источники:    
[1] https://handbook.mathpsy.com/?page_id=238    
[2] https://www.codingprof.com/5-ways-to-check-the-normality-of-residuals-in-r-examples/     
[3] https://milnepublishing.geneseo.edu/natural-resources-biometrics/chapter/chapter-7-correlation-and-simple-linear-regression/    
[4] https://www.codecamp.ru/blog/lm-function-in-r/    
