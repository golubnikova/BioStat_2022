---
title: "Продвинутые способы визуализации данных в R"
subtitle: "Визуализация и эксплораторный анализ данных"
author: "Дмитрий Серебренников"
date: "22.10.2022"
output: 
  html_document:
    code_folding: show
    df_print: paged
    highlight: pygments
    smooth_scroll: no
    theme: united
    toc: yes
    toc_depth: 3
    toc_float: yes
    toc_position: right

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Загрузим данные и пакеты

```{r}
# Если у вас не установлены какие-то из библиотек ниже, то установить их можно следующей командой. Эти библиотеки пригодятся нам во время занятия, но подгружать в library() мы их будем тогда, когда будем затрагивать соответствующую тему.
# install.packages(c('ggpubr', 'plotly', 'corrplot', 'corrr', 'ggfortify', 'pheatmap', 'factoextra', 'FactoMineR', 'ggbiplot'))

# Загрузим библиотеки
library(dplyr)
library(ggplot2)
library(ggpubr)

pima <- read.csv('pima.csv')

# Сделаем более детализированную переменную возрастных групп
pima <- pima %>% 
  mutate(
    diabetes_ch = as.character(diabetes),
    age_group = case_when(
      age < 31 ~ "21-30",
      age >= 31 & age < 41 ~ "31-40",
      age >= 41 & age < 51 ~ "41-50",
      age >= 51 & age < 61 ~ "51-60",
      age >= 61 ~ "60+"
    ))

table(pima$age_group)
```


...


# Plotly - Интерактивные графики

Plotly - это специальная библиотека для создания интерактивных графиков со своим синтаксисом (очень похожим на ggplot). С ней можно познакомиться [здесь](https://plotly.com/r/). 


## Основные правила синтаксиса

Загрузим библиотеку: 

```{r}
# install.packages("plotly")
# или 
# devtools::install_github("ropensci/plotly")
library(plotly)

skimr::skim(pima)
```

Разберём базовые примеры. Для более подробного изучения приглашаю [сюда](https://plotly.com/r/)

### Box plot:

```{r}
plot_ly(
  pima[pima$insulin != 0,],
  x = ~ insulin,
  color = ~ age_group,
  type = "box"
)
```

### Scatter plot

```{r}
plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose)
```

## Начнём усложнять

Добавим название и настройки маркеров: 

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
  x = ~ insulin,
  y = ~ glucose,
  marker = list(
    size = 10,
    # размер
    color = 'rgba(255, 182, 193, .9)',
    # Цвет внутри
    line = list(color = 'rgba(152, 0, 0, .8)',  # Цвет окружности
                width = 2)
  )
)   %>%
  layout(
    title = 'Отношение уровня глюкозы и инсулина в данных PIMA',
    yaxis = list(title = 'Уровень глюкозы',
                 zeroline = FALSE),  # Уберём выделения нулевых осей по y
    xaxis = list(title = 'Уровень инсулина',
                 zeroline = FALSE)) # Уберём выделения нулевых осей по y

```


Раскрасим по диабет-статусу:


```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
  x = ~ insulin,
  y = ~ glucose,
  color = ~diabetes_ch
)   %>%
  layout(
    title = 'Отношение уровня глюкозы и инсулина в данных PIMA',
    yaxis = list(title = 'Уровень глюкозы',
                 zeroline = FALSE),  # Уберём выделения нулевых осей по y
    xaxis = list(title = 'Уровень инсулина',
                 zeroline = FALSE)) # Уберём выделения нулевых осей по y

```


Добавим третью ось:

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0) & (pima$mass != 0),],
  x = ~ insulin,
  y = ~ glucose,
  z = ~mass,
  color = ~diabetes_ch
)   %>%
  layout(
    title = 'Отношение уровня глюкозы и инсулина в данных PIMA',
    yaxis = list(title = 'Уровень глюкозы',
                 zeroline = FALSE),  # Уберём выделения нулевых осей по y
    xaxis = list(title = 'Уровень инсулина',
                 zeroline = FALSE)) # Уберём выделения нулевых осей по y


```



### Palette / Цветовые гаммы

```{r}
# Со стандартной палеткой
plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose,
        color = ~age_group)

# С изменённой палеткой
plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose,
        color = ~age_group,
        colors = "Set1")
```

Палетки можно сделать самостоятельно (аналогичным образом это делается в ggplot): 

```{r}
pal <- c("red", "blue", "green")
pal <- setNames(pal, levels(gapminder_2007$age_group))

plot_ly(data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
        x = ~ insulin,
        y = ~ glucose,
        color = ~age_group,
        colors = pal)
```



### Цвет, размер точек и подписи к ним

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),], 
  x = ~insulin, 
  y = ~glucose,
  color = ~diabetes_ch, 
  size = ~mass,
  text = ~mass, 
  hoverinfo = "text")
```

### Анимированный график

Построим анимацию для разных возрастов. Она не будет особо осмысленна (т.к. делается обычно для данных по времени), но мы учимся:

```{r}
plot_ly(
  data = pima[(pima$insulin != 0) & (pima$glucose != 0),],
  x = ~insulin, 
  y = ~glucose, 
  color = ~diabetes_ch, 
  size = ~mass,
  text = ~mass, 
  frame = ~age, # По какой колонке нам сделать анимацию/интерактивность?
  hoverinfo = "text",
  type = 'scatter',
  mode = 'markers'
) 
```

### Почему мы не учим plotly в первую очередь?

Он не гибкий. Если ggplot понимает что вам нужно на ходу, то для plotly вам нужно подготовить данные в том виде в котором вы хотите их визуализировать.



*ЗАДАНИЕ*

В чанке ниже напишите код 3d графика ао осям age, mass, pressure:

```{r}

```





## Обход - ggplotly

Когда вы хотите представить итоговый результат в интерактивном формате (например, с помощью странички на сайте сделанной в Rmd), то её лучше писать в plotly. Однако, если вы работаете в формате эксплораторного анализа, то можно воспользоваться "обёрткой" plotly для вашего ggplot графика. Сделать это очень просто.

Сделаем и сохраним обычный график:

```{r}
plot <- pima %>% 
  filter(mass != 0 & triceps != 0) %>% 
  ggplot(aes(x=mass, y=triceps, color = diabetes)) + 
  geom_point(size=3) +
  theme_minimal()
```
 

"Обернём" наш график:

```{r}
ggplotly(plot)
```



*ЗАДАНИЕ*

В чанке ниже напишите код любого графика и сделайте его интерактивным:

```{r}

```




...





# Анализ корреляций

Когда мы говорим об отношениях переменных друг к другу, мы так или иначе приходим к языку корреляции. Т.е. статистической взаимосвязи двух или более случайных величин при которых изменения в одной переменной связаны с изменением во второй. 

На всякий случай привожу формулу рассчёта коэффициента корреляции Пирсона:
$r_{xy}=\frac{\Sigma(x_i-\bar{x})\times(y_i-\bar{y})}{\sqrt{\Sigma(x_i-\bar{x})^2\times\Sigma(y_i-\bar{y})^2}}$

Он может принимать значения от -1 (отрицательная связь), до 1 (положительная связь).

В R можно строить матрицу корреляций для всех численных переменных нашего датасета с помощью пакета corrplot. Загрузим его:

```{r}
library(corrplot)
```

Сама матрица строится в 2 этапа.

1. Получаем объект матрицы:
```{r}
# Для более "чистого" результата, избавляемся от ошибочных значений
pima_clear <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 ) %>% 
  select(is.integer | is.numeric) # Обратите внимание, в dplyr можно задавать выборку колонок через команды определения формата данных

head(pima_clear)
```

```{r}
# Получаем непосредственно матрицу
pima_cor <- cor(pima_clear)
pima_cor
```

2. Визуализируем её в corplot:
```{r}
corrplot(pima_cor, method = 'number')
```

У представления такой матрицы может быть ещё большое количество вариаций. Их можно посомтреть [здесь](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

Кроме того, я рекомендую посмотреть также на пакет [corrr](https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr).

В нём реализованы более дателизированные функции для анализа корреляций. Например функция ниже показывает не только корреляционные взаимосвязи, но и отношения близости переменных друг к другу с точки зрения сетевого анализа:

```{r}
library(corrr)

pima_cor %>% 
  network_plot(min_cor = .0)
```



...



# Heat map

## Анализ двух категориальных переменных

В базовом варианте, heat map используют для иллюстрации отношения двух категориальных переменных друг с другом. Для этого, нужно сначала агрегировать их значения:

```{r}
pima_aggr <- pima %>% 
  group_by(age_group, diabetes) %>% 
  summarise(N = n())
  
```

Визуализируем:

```{r}
pima_aggr %>% 
  ggplot(aes(x = age_group, y = diabetes, fill = N)) +
  geom_tile(color = "black") +
  geom_text(aes(label = N), color = "white", size = 4) +
  coord_fixed()
```


## Анализ наблюдений 

Второй способ использовать heat map - представлять сами наблюдения по какому-то универсальному признаку.

Например, мы можем применить процедуру стандартизации значений для всех наблюдений по численным переменным и визуализировать его. 

Стандартизируем значения:
```{r}
pima_clear_scaled <- scale(pima_clear)
head(pima_clear_scaled)
```








Визуализируем с помощью расширения к ggplot - ggfortify(). В чем проблема такого графика?

```{r}
library(ggfortify) 

autoplot(pima_clear_scaled)
```


...


# Методы уменьшения размерности

## Поиск схожих наблюдений. Лирическое отступление об основах кластерного анализа

### Иерархическая кластеризация (hierarchical clustering) 

Относится к семейству аггломеративных кластеризаций (agglomerative/agnes, есть ещё другой вид - divisive/diana), но является самой простой из них - не требует выделить число кластеров заранее. Нужна в первую очередь, чтобы увидеть сруктуру отнесения наблюдений к кластерам (и подумать о том, как вообще устроенны наши наблюдения).

Принцип работы - каждое наблюдение находит ближайшее себе по дистанции до него (по каждой из переменных). После нахождения первого соседа, начинатся следующий шаг, когда образовавшийся мини-кластер ищет другие ближайшие к ним кластеры/наблюдения. Так прохожит несколько шагов, пока все наблюдения не сольются в один кластер.

"Длинна" дистанции обозначает меру схожести наблюдений - Высоту/Height (Площадь Восстания ближе к ЕУ чем Кудрово).

Посмотрим визуально на [картинке](https://drive.google.com/file/d/1vzzHg4GwWd6bQKMx1Y_7yOs39NTEKHQo/view?usp=sharing).

...

Есть много способов измерять расстояние. Мы будем пользоваться базовым - Евклидовым расстоянием.


**Как с этим работать?**

1. Установик и загрузим библиотеку для визуализации кластеров
```{r}
library(factoextra)
```

2. Создаём матрицу дистанций
```{r}
pima_clear_dist <- dist(pima_clear_scaled, method = "euclidean")
as.matrix(pima_clear_dist)[1:6,1:6]
```


3. Высчитываем дендрограмму кластеров. Обратите внимание на аргумент method. Методов для кластеризации достаточно много. Мы используем один из базовых - "ward.D2"

```{r}
pima_clear_hc <- hclust(d = pima_clear_dist, 
                        method = "ward.D2")
```


4. Визуализируем (на больших данных может занять какое-то время)

```{r}
fviz_dend(pima_clear_hc, 
          cex = 0.1) # cex() - размер лейблов
```

Далее можно углубляться в способы визуализации и оценки таких результатов. Рекомендую посмотреть на учебник по [ссылке](https://xsliulab.github.io/Workshop/week10/r-cluster-book.pdf).


...



## Heat map + Tree map?

Мы затронули основы кластеризации, чтобы понять, как работать в тех случаях, когда нам нужно упорядочить данные, сгруппировав наблюдения. Совместив это с heat map мы можем выделить группы наблюдений по каким-либо признакам. 

Сделать это просто через пакет pheatmap.

Загрузим его.

```{r}
library(pheatmap)
```

Визуализируем heat map и tree map одновременно:

```{r}
pheatmap(pima_clear_scaled)
```

```{}
Что не так в этом графике?
```









## Principal component analysis (PCA)


Сейчас мы уже можем выделить определённые группы пациентов. Однако даже на таком маленьком объёме данных (напомню, мы работаем с 392 наблюдениями) график почти не читаем. Что делать?

Одно из самых популярных решений - метод главных компонент (principal component analysis или PCA). 


...


### Теория и *учебный* пример


Чтобы разбраться с методом рассмотрим какие две проблемы он решает и как он это делает:

1. Если в данных есть ряд скоррелированных переменных (например, в нашем случае - mass и triceps), то при анализе они не дадут провести корректные оценки данных и выделение паттернов. 

```{}
Почему?
```


2. Если в данных очень много наблюдений и переменных, то при разведывательном анализе необходимо сократить их размер, не потеряв в качестве и полноте интерпретаций. Эту задачу выполняют методы уменьшения размерности и PCA относится к ним.


...

Рассмотрим *учебный* пример. Допустим у нас в данных есть только колонки mass и triceps. Мы хотим с одной стороны снять корреляцию между ними, а с другой - уменьшить размерность, т.е. представить их одной переменной. 

```{}
Как нам это сделать?
```


...












**Мы можем сконструировать новые переменные из двух имеющихся и сняв корреляцию и уменьшив размерность!**

Давайте сначала ещё раз посмотрим на данные:

```{r}
# Подготовим данные
pima_example <- pima_clear %>% 
  select(mass, triceps)

# Визуализируем
ggplot() +
  geom_point(data = pima_example, aes(x = mass, y = triceps)) +
  theme_minimal()

# Что значит такое распределение точек? 
```



**Сконструируем переменные, в которых не будет корреляции и которые (делаем из бага фичу!) помогут уменьшить размерность**






Как нам сделать из двух переменных одну?







Если мы хотим привести две переменные к одной, то мы должны провести какое-то действие с этими переменными. Например, сложить. Давайте сделаем это:

```{r}
pima_example <- pima_example %>% 
  mutate(pc1 = mass + triceps) # Создаем первую главную компоненту
```

Давайте посмотрим, как на данных выглядят три типа значений. Те, которые близки к медиане по pc1, те которые близки к максимуму по pc1 и те, которые близки к минимуму по pc1.

Создадим дополнительную колонку по этим значениям:

```{r}
pima_example <- pima_example %>% 
  mutate(pc1_decile = ntile(pc1, 10))
```


А теперь визуализируем наш scatter plot соотношения mass и triceps, НО, раскрасим цветом значения первого, пятого, шестого и десятого децелей: 

Подготовим данные, добавив в них номинативную переменную исходных децелей:

```{r}
pima_example <- pima_example %>% 
  mutate(pc1_decile_ch = case_when(
    pc1_decile == 1 ~ "1",
    (pc1_decile == 5) | (pc1_decile == 6) ~ "5-6",
    pc1_decile == 10 ~ "10"
  ))
pima_example
```


```{r}
ggplot() +
  geom_point(data = pima_example, 
             aes(x = mass, 
                 y = triceps, 
                 color = pc1_decile_ch)) +
  theme_minimal()

# Что мы видим?
```

Такой результат мы получаем на сложении. А давайте теперь поэксперементируем с вычитанием:

```{r}
pima_example <- pima_example %>% 
  mutate(pc2 = mass - triceps) %>% # Создаём колонку для результатов вычитания
  mutate(pc2_decile = ntile(pc2, 10)) %>% # Находим её децили
    mutate(pc2_decile_ch = case_when( # Отмечаем номинативной переменной значения разных децелей
    pc2_decile == 1 ~ "1",
    (pc2_decile == 5) | (pc2_decile == 6) ~ "5-6",
    pc2_decile == 10 ~ "10"
  ))
```

Визуализируем:

```{r}
ggplot() +
  geom_point(data = pima_example, 
             aes(x = mass, 
                 y = triceps, 
                 color = pc2_decile_ch)) +
  theme_minimal()

# Что мы видим?
```


А теперь завершим нашу логику. Давайте оставим в данных только средние значения по каждому квантилю и потом нарисуем по ним линию тренда.

Создадим данные:

```{r}
pima_example_pc1 <- pima_example %>%
  group_by(pc1_decile) %>% # Группируем по квантилям (1:10)
  summarise(mass_pc1 = mean(mass), # В каждом квантиле находим среднее для mass
            triceps_pc1 = mean(triceps)) # В каждом квантиле находим среднее для triceps

# Сделаем тоже самое с pc2
pima_example_pc2 <- pima_example %>%
  group_by(pc2_quantile) %>%
  summarise(mass_pc2 = mean(mass),
            triceps_pc2 = mean(triceps))
```


Теперь визуализируем график по mass и triceps, НО добавив линию тренда по средним значениям pc1 и pc2 для mass и triceps:


```{r}
ggplot() +
  geom_point(data = pima_example,
             aes(x = mass, y = triceps)) + # Основные данные
  geom_smooth(data = pima_example_pc1, # Отмечаем линию тренда по pc1
              aes(x = mass_pc1, y = triceps_pc1), # Используем значения mass и triceps, которые являются средними для децилей pc1
              method=lm,
              color="Blue", fullrange = F,
              size = 2
              ) +
  geom_smooth(data = pima_example_pc2, # Аналогично с pc2
              aes(x = mass_pc2, y = triceps_pc2),
              method=lm, 
              orientation = "y", # Технический трюк. Просим R отстраивать линию вдоль оси y, а не x
              color="Green", fullrange = F,
              size = 2
              ) +
  theme_minimal()

```




```{}
А теперь, следим за руками...

А что если мы просто сделаем эти линии тренда нашими новыми координатными осями?
Т.е. вместо x будет pc1, вместо y - pc2, а точки перерисуются исходя из нашего нового представления. 
```


**Вспоминаем две проблемы, с которых мы начинали. Как такое преобразование помогает решить обе сложности?**



...


Чтобы не быть голословными, перерисуем график, но уже с новыми координатами - pc1 и pc2:


```{r}
ggplot() +
  geom_point(data = pima_example, aes(x = pc1, y = pc2)) +
  theme_minimal() 

# Догодайтесь, где теперь лежат средние линии наших старых координат (mass, triceps)?
```

```{}
Вопросы на понимание:

* А зачем мы это сделали? Какие следствия такого подхода?

* Если мы хотим выразить две наши переменные какой-то одной, то что мы выберем pc1 или pc2?
```


...


### Подойдём к вопросу более формально 

* Нам нужно найти оси, которые приведут корреляцию к нулю и при этом, создадут макисмальную полноту разброса переменных. Т.е. фактически, в нашей задаче мы решем два уравнения:

$ PC1 = (alpha_1 * mass) + (beta_1 * triceps) $
$ PC2 = (alpha_2 * mass) + (beta_2 * triceps) $

где alpha и beta - это коэффициенты, которые позволяют добиться минимальной скоррелированности mass и triceps. (читай - найти такой угол для новых осей координат, где в данных будет максимальный разброс при минимальной корреляции)

Воспользуемся специальной функцией для этого:

```{r}
pima_example <- pima_example %>% 
  select(mass, triceps) # Оставим только нужные нам две переменные

pima.pca <- prcomp(pima_example, 
                scale = T) # Нужно ли нормировать? TRUE|FALSE. Фактически, мы могли бы подать на функцию pima_example_scaled и тогда следовало поставить scale = F, веть нормирование в том датафрейме уже сделано!
```

Посмотрим на веса наших главных компонент:

```{r}
pima.pca$rotation
```

Rotation показывает коэффициенты разворота новых осей относительно старых. Таким образом, решением нашего уравнения будет:

$ PC1 = (0.707 * mass) + (-0.707 * triceps) $
$ PC2 = (0.707 * mass) + (0.707 * triceps) $

*(странность весов обусловлена тем, что мы работаем с учебным примером, где всего лишь 2 переменные)*


Нарисуем компоненты на графике:

```{r}
ggplot() +
  geom_point(data = pima.pca$x, # Новые переменные лежат здесь
             aes(x = PC1, y = PC2)) +
  theme_minimal() 
```


Но как функция высчитала эти цифры?
1. Она находит центр распределения данных по среднему двух переменных;
2. От него строит множество прямых линий с каждым разом делая угол наклона всё больше;
3. Для каждой линии считается сумма квадратов расстояний от всех точек в данных до линии;
4. Выбирается линия, где эта сумма будет минимальной. Она и становится новой главной компонентой;
5. Если алгоритм уже отобрал однй главную компоненту, он начинает отбирать следующую так, чтобы каждая следующая линия имела наименьшую корреляцию со всеми предыдущими;
6. Алгоритм останавливается, тогда, когда количесто найденных главных компонент не сравняется с количеством переменных.


```{}
Но в чём суть, если мы хотели уменьшить количество переменных, а оно (количество главных компонент) осталось таким же?

- Да, осталось, но только вот главные компоненты концептуально неравны друг другу. Каждая следующая из них объясняет меньше предыдущей. Считается, что алгоритм сработал хорошо, когда 70% вариации данных укладывается в 3 компоненты. 

...Т.е. компонент может быть десятки, но первые 3 агреггируют в себе большую часть сложности наших данных. 
```


Это легко заметить уже даже в случае трёх переменных (и трёх компонент). Рассмотрим прекрасный материал Гарика Мороза и соавторов по ссылке [здесь](http://math-info.hse.ru/f/2015-16/ling-mag-quant/lecture-pca.html).



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```{r}
# Кстати, мы теперь также можем делать 3d визуализацию с помощью plotly:
plot_ly(data = pima_clear, 
        x=~mass, 
        y=~pressure, 
        z=~triceps, 
        size = 1,
        type="scatter3d", mode="markers")
```
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


...


### Практика и реальный пример

Проведём PCA уже на полных данных pima

```{r}
# Загрузим библиотеки
library(FactoMineR)
library(ggbiplot)
```

Делаем PCA:

```{r}
pima_full.pca <- prcomp(pima_clear, 
                        scale = T) # Не забываем про стандартизацию!
```

Оценим результат. 

```{r}
summary(pima_full.pca)
```

Смотрим на "Cumulative Proportion". У нас в данных первые 4 главные компоненты объясняют 74% вариации данных. Посмотрим это на графике:

```{r}
fviz_eig(pima_full.pca, 
         addlabels = T, 
         ylim = c(0, 40))
```

На самом деле, это не слишком хороший результат, т.к. следующая конвенционально важная отметка в 90% достигается уже только на PC7. Бывают данные, которые не слишком хороши для PCA, но нельзя сказать, что у нас всё ужасно. Первые две компоненты объясняют 50% дисперсии. Вокруг них и будет сосредоточен основной анализ.


#### Анализ переменных по PCA


Мы можем начать анализировать, как наши переменные связаны с PC1 и PC2. Посмотрим на график ниже:

```{r}
fviz_pca_var(pima_full.pca, col.var = "contrib")
```

Подсказка для интерпретации графика [здесь](https://bioinfo4all.files.wordpress.com/2021/01/principal-component-analysis-pca-1.png?w=2048).

- Стрелки - средние значения переменных для PC1 (Dim1) и PC2 (Dim2). В скобках указаны проценты объяснённой дисперсии каждой из двух компонент. На каждую последующую PC всегда приходится всё меньше и меньше разброса в данных.
- Цвет и близость к кругу - насколько та или иная переменная вносит вклад в анализируемые главные компоненты
- Направление - относительная мера близости переменных. Если стрелки расходятся в прямо-противоположные стороны, то переменные отрицательно скоррелированы внутри представленных главных компонент.

В данных мы видим три группы переменных:
* age, pregnant.
* mass, triceps, pedigree
* остальные


```{}
Теперь оценим PC1 и PC2 в целом. Что "схватили" первая и вторая компоненты? какой разброс мы видим по осям x и y?
```


...


Мы также можем отдельно посмотреть на, например, топ 3 самых важных переменных с т.зр. их вариации в PC1 и PC2:

```{r}
fviz_pca_var(pima_full.pca, 
             select.var = list(contrib = 3), # Задаём число компонент здесь 
             col.var = "contrib")
```



```{}
У самих по себе главных компонент есть одна очень большая проблема с точки зрения их анализа. Какая?
```



Посмотрим из чего состоят 1, 2 и 3 главные компоненты:

```{r}
fviz_contrib(pima_full.pca, choice = "var", axes = 1, top = 24) # 1
fviz_contrib(pima_full.pca, choice = "var", axes = 2, top = 24) # 2
fviz_contrib(pima_full.pca, choice = "var", axes = 3, top = 24) # 3
```





#### Анализ наблюдений по PCA

Помимо переменных мы можем анализировать также и наблюдения, искать в них кластеры и корреляцию с переменными в целом. Для этого используется biplot. Bi потому, что на нем одновременно изображены и точки, и переменные

```{r}
# Загрузим библиотеку
library(ggbiplot)
```

Сделаем biplot:

```{r}
ggbiplot(pima_full.pca, 
         scale=0, alpha = 0.1) + 
  theme_minimal()
```


Более осмысленным biplot становится при использовании кластерных методов, с помощью которых мы можем разделить наблюдения на группы. Посмотрим, наблюдается ли разница между группами по diabetes:

```{r}
# Сделаем корректные данные для группировки по diabetes.
pima_clear_with_ch <- pima %>% 
  filter(glucose != 0 & pressure != 0 & triceps != 0 & insulin != 0 & mass != 0 & age != 0 )

# Визуализируем с группировкой по diabetes (для этого переменную нужно сделать фактором)
ggbiplot(pima_full.pca, 
         scale=0, 
         groups = as.factor(pima_clear_with_ch$diabetes), 
         ellipse = T,
         alpha = 0.2) +
  theme_minimal()
```

А что с возрастными группами:

```{r}
ggbiplot(pima_full.pca, 
         scale=0, 
         groups = as.factor(pima_clear_with_ch$age_group), 
         ellipse = T,
         alpha = 0.2) +
  theme_minimal()
```


Для дальнейшего ознакомления с PCA я рекомендую посмотреть следующие туториалы:
* https://bioinfo4all.wordpress.com/2021/01/31/tutorial-6-how-to-do-principal-component-analysis-pca-in-r/
* https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff
* https://juliasilge.com/blog/best-hip-hop/
* https://juliasilge.com/blog/cocktail-recipes-umap/
* http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/


...



# Завершение. А если графики используются не для эксплораторного анализа, а для презентации результатов?


В визуализации данных есть своя теория и свои исследования воприятия разных графиков (например что-то вы можете почитать на [data-to-viz в блоге](https://www.data-to-viz.com/caveats.html) или у [Анастасии Кузнецовой](https://nastengraph.medium.com/)).

Когда график для вас - способ представить результат вашим коллегам или широкой публике, его стоит делать исходя из несколько других соображений, чем когда вы делаете это при эксплораторном анализе. По этой причине, при подготовке графика следует учесть следующие принципы:

1. Оцените насколько хорошо ваши данные подходят типу графика. 

2. Делайте фокус на чем-то одном: общих паттернах или деталях. Исходя из этого стоит выбирать тип графика.

3. Агрегируйте большие объемы данных при визуализации.

4. Правильно выбирайте палетки. Учитывайте то, как их могут читать дальтоники или, например, акцентируют ли цвета внимание читателя на том, что вам нужно?

5. Не делайте график впечатлительным без необходимости. Эффектность в простоте.

6. Убедитесь, что график соответствует интуитивным конвенциям восприятия (у него не перевёрнуты оси, они не обрезаны (но иногда это позволительно), у данных указан источник, все подписи унифицированны и проч.)

7. При интерпретации результатов помните - график показывает только то, что он показывает. Ни один график не показывает вам причинных эффектов. Только связи. Correlation != Causation.  

8. Помните, что цель любого хорошего графика – рассказать историю (и убедить вас в ней). 


...


Я попытался рассказать историю на наших данных на графике ниже. Попробуйте и вы!

```{r}
plot <- pima %>% 
  mutate(
    age_group = factor(age_group, levels = c("21-30", "31-40", "41-50", "51-60", "60+" )),
    diabetes = case_when(
      diabetes == 'pos' ~ "Diabet-Positive",
      diabetes == 'neg' ~ "Diabet-Negative"
    ) # Переводим в фактор, для корректной последовательности категорий
    ) %>% 
  filter(insulin != 0 & glucose != 0) %>% 
  ggplot(aes(x=insulin, y=glucose, color = age_group)) + 
  geom_point(size = 3, alpha = 0.8) + 
  facet_grid(. ~ diabetes) +
  scale_color_brewer(palette = 'OrRd') +
  ggtitle('Indian Women Diabets') +
  labs(y = 'Plasma glucose concentration (log10)', x = '2-Hours serum insulin (mu U\\ml) (log10)') + 
  guides(color = guide_legend(title = 'Age Groups')) +
  scale_x_log10() + scale_y_log10() +
  theme_minimal() 

plot
```


# Полезные ссылки по теме визуализации данных

* [R Graph Galery](https://r-graph-gallery.com/index.html)

* Ссылки на Телеграмм каналы:
  - https://t.me/revealthedata
  - https://t.me/nastengraph
  - https://t.me/data_publication
  - https://t.me/leftjoin



